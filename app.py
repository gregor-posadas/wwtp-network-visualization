# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Isy1PIqP2P8kxi4kPrMobZtKAUYDPFr
"""

# app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from scipy import stats
from statsmodels.stats.multitest import multipletests
from sklearn.utils import resample
import itertools
import matplotlib.pyplot as plt
import networkx as nx
from matplotlib.colors import to_rgba
import matplotlib.patches as mpatches
import textwrap
from matplotlib.gridspec import GridSpec
from matplotlib.offsetbox import DrawingArea, TextArea, HPacker, VPacker, AnnotationBbox
import matplotlib

# Prevent matplotlib from trying to use any Xwindows backend.
matplotlib.use('Agg')

# -------------------------------
# Data Processing Functions
# -------------------------------

def bootstrap_correlations(df, n_iterations=500, method='pearson'):
    correlations = []
    for i in range(n_iterations):
        df_resampled = resample(df)
        corr_matrix = df_resampled.corr(method=method)
        correlations.append(corr_matrix)
    median_corr = pd.concat(correlations).groupby(level=0).median()
    return median_corr

def calculate_p_values(df, method='pearson'):
    p_values = pd.DataFrame(np.ones((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)
    for col1, col2 in itertools.combinations(df.columns, 2):
        try:
            _, p_val = stats.pearsonr(df[col1], df[col2])
            p_values.at[col1, col2] = p_val
            p_values.at[col2, col1] = p_val
        except Exception:
            p_values.at[col1, col2] = 1
            p_values.at[col2, col1] = 1
    return p_values

def correct_p_values(p_values):
    _, corrected, _, _ = multipletests(p_values.values.flatten(), alpha=0.05, method='fdr_bh')
    corrected_p = pd.DataFrame(corrected.reshape(p_values.shape), index=p_values.index, columns=p_values.columns)
    return corrected_p

def find_common_parameters(dataframes):
    """
    Identify parameters (columns) that are common across multiple DataFrames.
    """
    if not dataframes:
        return []

    # Start with all columns from the first DataFrame
    common_columns = set(dataframes[0].columns)

    # Intersect with columns from the remaining DataFrames
    for df in dataframes[1:]:
        common_columns &= set(df.columns)

    # Exclude the 'date' column
    common_columns.discard('date')

    return list(common_columns)

def remove_outliers_zscore(df, threshold=3):
    """
    Remove outliers from a DataFrame using the Z-score method.
    """
    st.write("Applying Z-Score Method to filter outliers...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    z_scores = np.abs(stats.zscore(df[numeric_cols], nan_policy="omit"))
    mask = (z_scores < threshold).all(axis=1)
    filtered_df = df[mask]
    st.write(f"Outliers removed: {len(df) - len(filtered_df)}")
    return filtered_df

def validate_correlation_matrix(df, n_iterations=500, alpha=0.05):
    st.write(f"DataFrame shape: {df.shape}")
    """
    Validate correlations using bootstrapping and p-value correction.
    Returns a filtered correlation matrix with only significant values.
    """
    st.write("Bootstrapping correlation matrices...")

    # Ensure all columns are numeric
    df = df.apply(pd.to_numeric, errors='coerce')
    df = df.dropna(axis=1, how='all')  # Drop columns that are entirely non-numeric or NaN

    pearson_corr = bootstrap_correlations(df, n_iterations=n_iterations, method='pearson')
    spearman_corr = bootstrap_correlations(df, n_iterations=n_iterations, method='spearman')
    kendall_corr = bootstrap_correlations(df, n_iterations=n_iterations, method='kendall')

    avg_corr_matrix = (pearson_corr + spearman_corr + kendall_corr) / 3

    st.write("Calculating and correcting p-values...")
    p_values = calculate_p_values(df, method='pearson')
    corrected_p_values = correct_p_values(p_values)

    sig_mask = (corrected_p_values < alpha).astype(int)
    filtered_corr_matrix = avg_corr_matrix.where(sig_mask > 0).fillna(0)

    st.write("Correlation matrix validated and filtered based on significance.")
    return filtered_corr_matrix

# -------------------------------
# Visualization Functions
# -------------------------------

def generate_heatmap(df, title, labels):
    """
    Generate a heatmap and return the filtered correlation matrix.
    """
    filtered_corr_matrix = validate_correlation_matrix(df)
    parameter_order = sorted(filtered_corr_matrix.index)
    filtered_corr_matrix = filtered_corr_matrix.loc[parameter_order, parameter_order]

    np.fill_diagonal(filtered_corr_matrix.values, 1)

    st.subheader(title)
    fig = px.imshow(
        filtered_corr_matrix,
        text_auto=".2f",
        color_continuous_scale="RdBu",
        zmin=-1,
        zmax=1,
        labels={"x": labels[0], "y": labels[1], "color": "Correlation Coefficient"},
        title=title,
    )

    fig.update_layout(
        title=dict(font=dict(size=20), x=0.5),
        xaxis=dict(tickangle=45, title=None, tickfont=dict(size=12)),
        yaxis=dict(title=None, tickfont=dict(size=12)),
        autosize=False,
        width=800,
        height=600,
        margin=dict(l=100, r=100, t=100, b=100),
    )

    st.plotly_chart(fig)
    return filtered_corr_matrix

def generate_network_diagram_streamlit(labels, correlation_matrices, parameters, globally_shared=True):
    """
    Generate a parameter-based network diagram.
    If globally_shared is True, use globally shared parameters.
    Otherwise, use locally shared parameters for each edge.
    """
    G = nx.MultiGraph()
    diagram_type = "Globally Shared" if globally_shared else "Locally Shared"

    st.subheader(f"{diagram_type} Network Diagram")

    # Collect data for edge summary boxes
    edge_summaries = []

    for i in range(len(labels) - 1):
        st.write(f"Processing connection: {labels[i]} → {labels[i + 1]}")

        # Retrieve the filtered correlation matrix for this pair
        filtered_corr_matrix = correlation_matrices[i]

        # Track added edges to avoid duplicates
        added_edges = set()

        if globally_shared:
            parameters_to_use = parameters  # Use the set of globally shared parameters
        else:
            parameters_to_use = parameters[i]  # Use the list of parameters for this edge

        node1 = labels[i]
        node2 = labels[i + 1]

        edge_summary = {
            'nodes': (node1, node2),
            'parameters': []
        }

        for parameter in parameters_to_use:
            edge_key = (node1, node2, parameter)

            param1 = f"{parameter}_{node1}"
            param2 = f"{parameter}_{node2}"

            if param1 in filtered_corr_matrix.index and param2 in filtered_corr_matrix.columns:
                corr_value = filtered_corr_matrix.loc[param1, param2]

                if corr_value == 0 or edge_key in added_edges:
                    continue

                # Add nodes and edge
                G.add_node(node1, label=node1)
                G.add_node(node2, label=node2)

                G.add_edge(
                    node1,
                    node2,
                    parameter=parameter,
                    correlation=corr_value,
                    weight=abs(corr_value),
                    key=parameter  # Use the parameter as the key for multi-edges
                )
                added_edges.add(edge_key)

                # Add to edge summary
                edge_summary['parameters'].append((parameter, corr_value))

        if edge_summary['parameters']:
            edge_summaries.append(edge_summary)

    if G.number_of_nodes() == 0:
        st.warning("No nodes to display in the network diagram.")
        return

    # Create a figure with GridSpec: 2 rows (network diagram and text boxes)
    fig = plt.figure(figsize=(18, 18))  # (width, height)
    gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.3)

    # Upper subplot for the network diagram
    ax_network = fig.add_subplot(gs[0, 0])

    # Adjust layout
    if globally_shared:
        pos = nx.kamada_kawai_layout(G)
    else:
        pos = nx.spring_layout(G, k=0.15, iterations=200, seed=42)  # Adjusted 'k' for closer nodes

    # Draw nodes
    node_colors = ["lightblue"] * len(G.nodes())
    nx.draw_networkx_nodes(G, pos, node_size=8000, node_color=node_colors, ax=ax_network)

    # Wrap node labels
    max_label_width = 10  # Adjust as needed
    formatted_labels = {}
    for node in G.nodes():
        label_text = G.nodes[node]['label'].replace("_", " ")
        wrapped_label = "\n".join(textwrap.wrap(label_text, width=max_label_width))
        formatted_labels[node] = wrapped_label

    # Draw labels with formatted labels
    nx.draw_networkx_labels(G, pos, labels=formatted_labels, font_size=12, font_weight="bold", ax=ax_network)

    # Assign unique colors to parameters
    unique_parameters = list(set(d['parameter'] for u, v, k, d in G.edges(keys=True, data=True)))
    num_params = len(unique_parameters)
    base_colors = plt.cm.tab10.colors  # You can choose other colormaps if you have more than 10 parameters
    if num_params > len(base_colors):
        base_colors = plt.cm.tab20.colors  # Use a colormap with more colors
    parameter_colors = dict(zip(unique_parameters, base_colors[:num_params]))

    # Function to adjust color intensity based on correlation strength
    def adjust_color_intensity(base_color, corr_value):
        rgba = to_rgba(base_color)
        intensity = 1.0  # Keep alpha at 1 for consistency
        adjusted_color = (rgba[0], rgba[1], rgba[2], intensity)
        return adjusted_color

    # Draw edges with curvature to avoid overlaps
    num_edges = len(G.edges(keys=True))
    curvature_values = np.linspace(-0.5, 0.5, num_edges)  # Adjusted for better curvature

    for idx, (u, v, key, d) in enumerate(G.edges(data=True, keys=True)):
        curvature = curvature_values[idx] if num_edges > 1 else 0.2
        corr_value = d['correlation']
        parameter = d['parameter']
        base_color = parameter_colors[parameter]
        edge_color = adjust_color_intensity(base_color, corr_value)

        # Choose line style based on correlation sign
        style = 'solid' if corr_value >= 0 else 'dashed'

        # Draw the edge
        nx.draw_networkx_edges(
            G,
            pos,
            edgelist=[(u, v, key)],
            connectionstyle=f"arc3,rad={curvature}",
            edge_color=[edge_color],
            width=d["weight"] * 5,
            style=style,
            ax=ax_network
        )

    # Set title for the network diagram
    ax_network.set_title(f"{diagram_type} Parameter-Based Network Diagram", fontsize=16, pad=20, weight="bold")

    # Create consolidated edge summary text box
    section_boxes = []

    for summary in edge_summaries:
        node1, node2 = summary['nodes']
        process_pair_title = f"{node1} → {node2}"
        title_area = TextArea(process_pair_title, textprops=dict(color='black', size=12, weight='bold'))

        # Create content boxes for parameters
        content_boxes = []
        for param, corr in summary['parameters']:
            color = parameter_colors[param]
            da = DrawingArea(20, 10, 0, 0)
            da.add_artist(mpatches.Rectangle((0, 0), 20, 10, fc=color, ec='black'))
            line_text = f"{param}: {corr:.2f}"
            ta = TextArea(line_text, textprops=dict(color='black', size=10))
            hbox = HPacker(children=[da, ta], align="center", pad=0, sep=5)
            content_boxes.append(hbox)
        # Pack the title and parameters vertically
        section_box = VPacker(children=[title_area] + content_boxes, align="left", pad=0, sep=2)
        section_boxes.append(section_box)

    # Pack all sections into one box with spacing
    all_sections_box = VPacker(children=section_boxes, align="left", pad=0, sep=10)

    # Diagram Interpretation
    interpretation_text = (
        "Diagram Interpretation:\n"
        "• Nodes represent collected samples.\n"
        "• Edges represent significant correlations between parameters.\n"
        "• Edge colors correspond to parameters (see edge summaries below).\n"
        "• Solid lines: Positive correlations.\n"
        "• Dashed lines: Negative correlations.\n"
        "• Edge thickness reflects correlation strength."
    )
    interpretation_area = TextArea(interpretation_text, textprops=dict(fontsize=12))

    # Combine edge summaries and interpretation
    combined_box = VPacker(children=[all_sections_box, interpretation_area], align="left", pad=20, sep=20)

    # Create the lower subplot for text boxes
    ax_text = fig.add_subplot(gs[1, 0])
    ax_text.axis("off")  # Hide the axes

    ab = AnnotationBbox(
        combined_box,
        (0.5, 0.5),  # Center of the subplot
        xycoords='axes fraction',
        box_alignment=(0.5, 0.5),
        bboxprops=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5', alpha=0.9)
    )
    ax_text.add_artist(ab)

    plt.tight_layout()
    st.pyplot(fig)

# -------------------------------
# Main Streamlit App
# -------------------------------

def main():
    st.set_page_config(page_title="WWTP Unit Processes Network Visualization", layout="wide")
    st.title("WWTP Unit Processes Network Visualization")
    st.write("Upload your data files to generate correlation heatmaps and network diagrams.")

    # File uploader
    uploaded_files = st.file_uploader(
        "Choose CSV or Excel files",
        accept_multiple_files=True,
        type=['csv', 'xlsx', 'xls']
    )
    process_labels = []
    dataframes = []
    edge_summaries = []

    if uploaded_files:
        for idx, uploaded_file in enumerate(uploaded_files):
            st.subheader(f"Process File {idx + 1}: {uploaded_file.name}")
            try:
                # Read the uploaded file
                if uploaded_file.name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(uploaded_file)
                else:
                    df = pd.read_csv(uploaded_file)

                df.columns = df.columns.str.lower().str.strip()
                if 'date' not in df.columns:
                    st.error(f"The file **{uploaded_file.name}** does not contain a 'date' column.")
                    st.stop()

                df['date'] = pd.to_datetime(df['date'], errors='coerce')
                df = df.dropna(subset=["date"])
                df = remove_outliers_zscore(df)
                dataframes.append(df)

                # Prompt user for a label for this process
                label = st.text_input(
                    f"Enter a label for **{uploaded_file.name}**:",
                    value=uploaded_file.name.split('.')[0],
                    key=f"label_{idx}"
                )
                process_labels.append(label)
            except Exception as e:
                st.error(f"Error processing file **{uploaded_file.name}**: {e}")
                st.stop()

        if len(dataframes) < 2:
            st.warning("Please upload at least two files to generate diagrams.")
            st.stop()

        # Identify common parameters
        common_params = find_common_parameters(dataframes)
        if not common_params:
            st.error("No common parameters found across all uploaded files.")
            st.stop()

        st.success(f"Common parameters identified: {', '.join(common_params)}")

        # Generate heatmaps and store correlation matrices
        correlation_matrices = []
        parameters_per_edge = []
        for i in range(len(dataframes) - 1):
            st.markdown(f"### Heatmap: **{process_labels[i]}** vs **{process_labels[i + 1]}**")
            df1 = dataframes[i][['date'] + common_params]
            df2 = dataframes[i + 1][['date'] + common_params]
            merged_df = pd.merge(
                df1, df2, on="date",
                suffixes=(f"_{process_labels[i]}", f"_{process_labels[i + 1]}")
            )
            merged_df = merged_df.drop(columns=["date"], errors="ignore")
            merged_df = merged_df.replace([np.inf, -np.inf], np.nan)
            merged_df = merged_df.dropna()
            numeric_columns = merged_df.select_dtypes(include=[np.number]).columns
            merged_df = merged_df[numeric_columns]

            filtered_corr_matrix = generate_heatmap(
                merged_df,
                f"Correlation Coefficient Heatmap: {process_labels[i]} vs {process_labels[i + 1]}",
                (process_labels[i], process_labels[i + 1]),
            )
            correlation_matrices.append(filtered_corr_matrix)

            # Identify parameters contributing to the correlation
            shared_params = []
            for param in common_params:
                infl_param = f"{param}_{process_labels[i]}"
                ode_param = f"{param}_{process_labels[i + 1]}"
                if infl_param in filtered_corr_matrix.index and ode_param in filtered_corr_matrix.columns:
                    if filtered_corr_matrix.loc[infl_param, ode_param] != 0:
                        shared_params.append(param)
            parameters_per_edge.append(shared_params)

        # Identify globally shared parameters
        globally_shared_parameters = set(parameters_per_edge[0])
        for params in parameters_per_edge[1:]:
            globally_shared_parameters &= set(params)

        st.markdown(f"**Globally shared parameters across all node pairs:** {', '.join(globally_shared_parameters) if globally_shared_parameters else 'None'}")
        if not globally_shared_parameters:
            st.error("No globally shared parameters found.")
            st.stop()

        # Buttons to generate network diagrams
        col1, col2 = st.columns(2)
        with col1:
            if st.button("Generate Globally Shared Network Diagram"):
                generate_network_diagram_streamlit(
                    process_labels,
                    correlation_matrices,
                    globally_shared_parameters,
                    globally_shared=True
                )

        with col2:
            if st.button("Generate Locally Shared Network Diagram"):
                generate_network_diagram_streamlit(
                    process_labels,
                    correlation_matrices,
                    parameters_per_edge,
                    globally_shared=False
                )

    st.markdown("---")
    st.markdown("### Instructions:")
    st.markdown("""
    1. **Upload Files:** Click on the "Browse files" button to upload your CSV or Excel files. Ensure each file contains a 'date' column and consistent parameters across files.
    2. **Label Processes:** For each uploaded file, enter a descriptive label (e.g., "Influent", "Aeration", "Effluent").
    3. **Generate Diagrams:** Once at least two files are uploaded and labeled, click on the buttons to generate either globally shared or locally shared network diagrams.
    4. **Interpret Results:** View the heatmaps and network diagrams to analyze correlations between parameters across different processes.
    """)

    st.markdown("""
    ---
    **Note:** This application requires an internet connection to upload files and generate visualizations.
    """)

# -------------------------------
# Run the Streamlit App
# -------------------------------

if __name__ == "__main__":
    main()